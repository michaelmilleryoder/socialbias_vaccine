{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c619ab-2073-4c1b-b9db-451b49207fc7",
   "metadata": {},
   "source": [
    "# Run PMI to find most associated words with identity terms in each corpus split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fe0dd-3241-453d-8dfe-911b9d56995c",
   "metadata": {},
   "source": [
    "## Load, process data, calculate cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19df8ba1-4c63-449a-a204-577d8024389f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0_anti-bot_sents', '0_anti-human_sents', '0_pro-bot_sents', '0_pro-human_sents'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data (tweet texts)\n",
    "import os\n",
    "\n",
    "split_type = '0_pro_anti_bot_human'\n",
    "dirpath = os.path.join('/home/huixiann/2022_socialbias_vaccine/michael/SAGE/py-sage/input/', split_type)\n",
    "processed = {}\n",
    "for fname in sorted(os.listdir(dirpath)):\n",
    "    fpath = os.path.join(dirpath, fname)\n",
    "    with open(fpath, 'r') as f:\n",
    "        processed[fname.split('.')[0]] = [[tok for tok in doc.split() if not tok.startswith('http')] for doc in f.read().splitlines()]\n",
    "        \n",
    "processed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b70260a2-722f-4473-863a-0ad9cc8e5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary of raw word co-occurrences (co-occur if the words occur ../identities.jsone same document)\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "freq_threshold = 100\n",
    "\n",
    "def process_section(section_parts):\n",
    "    name, section = section_parts\n",
    "    print(name)\n",
    "    sec_cooccurrences = defaultdict(int) # (word1, word2): n_times_co-occurs\n",
    "    sec_combination_word_freqs = defaultdict(int) # word1: n_times_occurs_in_combinations\n",
    "    sec_total_combinations = 0\n",
    "\n",
    "    counts = Counter([w for doc in section for w in doc])\n",
    "    sec_word_freqs = Counter({k: c for k,c in counts.items() if c >= freq_threshold})\n",
    "    \n",
    "    for doc in tqdm(section):\n",
    "        doc_toks = [w for w in doc if w in sec_word_freqs] # filters by freq\n",
    "\n",
    "        for pair in list(itertools.combinations(doc_toks, 2)):\n",
    "            sec_cooccurrences[tuple(sorted(pair))] += 1\n",
    "            sec_combination_word_freqs[pair[0]] += 1\n",
    "            sec_combination_word_freqs[pair[1]] += 1\n",
    "            sec_total_combinations += 1\n",
    "            \n",
    "    return sec_cooccurrences, sec_word_freqs, sec_combination_word_freqs, sec_total_combinations\n",
    "\n",
    "with Pool(len(processed)) as p:\n",
    "    results = list(tqdm(p.imap(process_section, sorted(processed.items())), total=len(processed)))\n",
    "\n",
    "cooccurrences = {}\n",
    "word_freqs = {}\n",
    "combination_word_freqs = {}\n",
    "total_combinations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d955ad-c0d6-471e-adea-17718c892b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result, section_parts in zip(results, processed.items()):\n",
    "    sec_cooccurrences, sec_word_freqs, sec_combination_word_freqs, sec_total_combinations = result\n",
    "    name = section_parts[0]\n",
    "    \n",
    "    cooccurrences[name] = sec_cooccurrences\n",
    "    word_freqs[name] = sec_word_freqs\n",
    "    combination_word_freqs[name] = sec_combination_word_freqs\n",
    "    total_combinations[name] = sec_total_combinations\n",
    "\n",
    "# Old, without multiprocessing\n",
    "# for name, section in processed.items():\n",
    "#     print(name)\n",
    "#     cooccurrences[name] = defaultdict(int) # (word1, word2): n_times_co-occurs\n",
    "#     combination_word_freqs[name] = defaultdict(int) # word1: n_times_occurs_in_combinations\n",
    "#     total_combinations[name] = 0\n",
    "\n",
    "#     counts = Counter([w for doc in section for w in doc])\n",
    "#     word_freqs[name] = Counter({k: c for k,c in counts.items() if c >= freq_threshold})\n",
    "# #     word_freqs[name] = Counter({k: c for k,c in counts.items() if c >= cooccurrence_min_freq}) # must occur that many times by themselves\n",
    "    \n",
    "#     for doc in tqdm(section):\n",
    "#         doc_toks = [w for w in doc if w in word_freqs[name]] # filters by freq\n",
    "\n",
    "#         for pair in list(itertools.combinations(doc_toks, 2)):\n",
    "#             cooccurrences[name][tuple(sorted(pair))] += 1\n",
    "#             combination_word_freqs[name][pair[0]] += 1\n",
    "#             combination_word_freqs[name][pair[1]] += 1\n",
    "#             total_combinations[name] += 1\n",
    "\n",
    "# #     cooccurrences[name] = {pair: count for pair, count in cooccurrences[name].items() if count >= cooccurrence_min_freq}\n",
    "    \n",
    "#     print(len(combination_word_freqs[name]))\n",
    "#     print(len(cooccurrences[name]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4158faa8-6611-4530-aadc-0c8c9f59a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def pmi(words, word_freqs, cooccurrences, n):\n",
    "    numerator = n * cooccurrences[words]\n",
    "    if numerator == 0:\n",
    "        return 0\n",
    "    denominator = word_freqs[words[0]] * word_freqs[words[1]]\n",
    "    return math.log(numerator/denominator, 2)\n",
    "\n",
    "def top_pmi(word, word_freqs, cooccurrences, n):\n",
    "    # Returns top co-occurring words with a specified word based on PMI\n",
    "    \n",
    "    cooccurring_words = []\n",
    "    \n",
    "    pairs = [pair for pair in cooccurrences.keys() if word in pair and pair != (word, word)]  # all words that co-occur\n",
    "    \n",
    "    for pair in pairs:\n",
    "        other_word = [w for w in pair if w != word][0]\n",
    "        cooccurring_words.append((other_word, pmi(pair, word_freqs, cooccurrences, n)))\n",
    "        \n",
    "    return sorted(cooccurring_words, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559becf6-49f8-4d96-b289-af966d46ce7a",
   "metadata": {},
   "source": [
    "## View top associated terms with terms of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dad3fb-4873-4659-bf3b-beb284b9ea7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gender/sexuality', 'age', 'race/ethnicity/nationality', 'religion', 'class', 'medical'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load identity terms (terms of interest)\n",
    "import json\n",
    "\n",
    "identities_fpath = '../identities.json'\n",
    "with open(identities_fpath) as f:\n",
    "    identities = json.load(f)\n",
    "identities.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659d038-4631-4622-a670-50133c023298",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender/sexuality\n",
      "woman\n",
      "0_anti-bot_sents: igbo, inspires, yogesh, serio, equ\n",
      "0_anti-human_sents: igbo, serio, equ, nigerian, seizures\n",
      "0_pro-bot_sents: serio, supermarket, immunologist, lakhs, peddling\n",
      "0_pro-human_sents: seizures, pleads, 6am, trek, oakeshott\n",
      "\n",
      "women\n",
      "0_anti-bot_sents: malian, ðŸ¤¥, #vaccinefor, brillian, #pmnarendramodi\n",
      "0_anti-human_sents: childbearing, #womenshea, brillian, fear-mongering, post-menopausal\n",
      "0_pro-bot_sents: consult, lactating, periods, insufficient, grannies\n",
      "0_pro-human_sents: s-only, consult, kenyan, sexuall, #pregnant\n",
      "\n",
      "man\n",
      "0_anti-bot_sents: shepherd, missile, undergoing, burning, buddy\n",
      "0_anti-human_sents: connaught, mulroney, capillary, missile, hesitated\n",
      "0_pro-bot_sents: erroneously, laquitta, criticize, willis, appointmen\n",
      "0_pro-human_sents: vara, laquitta, willis, deceased, #worldpraisepmikpolicies\n",
      "\n",
      "men\n",
      "0_anti-bot_sents: poverty, whic, prostate, ramai, bijwerkingen\n",
      "0_anti-human_sents: unknowingly, poverty, whic, beaten, syphilis\n",
      "0_pro-bot_sents: uss, servicewomen, wrapping, 540, essex\n",
      "0_pro-human_sents: vara, servicewomen, uss, 540, wrapping\n",
      "\n",
      "girl\n",
      "0_anti-bot_sents: paralyzed, c0, perpetrated, accuses, trips\n",
      "0_anti-human_sents: paralyzed, perpetrated, accuses, c0, trips\n",
      "0_pro-bot_sents: elementary, denounce, zoom, hijab, trips\n",
      "0_pro-human_sents: sexuall, denounce, elementary, hijab, zoom\n",
      "\n",
      "girls\n",
      "0_anti-bot_sents: #arrestmohammedzubair, ludhiana, gg, responsibility, lesson\n",
      "0_anti-human_sents: nazis, ludhiana, srinagar, boys, #puducherry\n",
      "0_pro-bot_sents: ðŸ˜”, nights, grocery, hijab, manatee\n",
      "0_pro-human_sents: nights, boys, ðŸ˜”, nss, sikh\n",
      "\n",
      "gal\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "boy\n",
      "0_anti-bot_sents: brief, bonus, mystery, 1m, unexpected\n",
      "0_anti-human_sents: hardest, hospitalised, youngest, mystery, sticker\n",
      "0_pro-bot_sents: ali, scenario, brief, throwing, iran\n",
      "0_pro-human_sents: t-shirt, ali, wounded, scenario, throwing\n",
      "\n",
      "boys\n",
      "0_anti-bot_sents: kohli, hearts, trailer, hs, risky\n",
      "0_anti-human_sents: trailer, antifa, blm, capitol, qanon\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: ðŸ˜¹, sex, girls, surgeries, hearts\n",
      "\n",
      "mother\n",
      "0_anti-bot_sents: lalwani, 411009, availabl, herself, reserved\n",
      "0_anti-human_sents: herself, availabl, 302001, 1918, reserved\n",
      "0_pro-bot_sents: passover, elementary, girl, zoom, 1918\n",
      "0_pro-human_sents: jessie, trenches, ww1, elementary, uncles\n",
      "\n",
      "mum\n",
      "0_anti-bot_sents: bkc, inspires, relief, mishra, #teamhalo\n",
      "0_anti-human_sents: relief, befo, 2Ã¨me, ken, hug\n",
      "0_pro-bot_sents: england's, van-tam, jonathan, 78, #joysms\n",
      "0_pro-human_sents: england's, van-tam, jonathan, vacuum, tha\n",
      "\n",
      "mom\n",
      "0_anti-bot_sents: pre-existing, randomized, hoax, placebo, self\n",
      "0_anti-human_sents: pre-existing, randomized, cooper, neglected, hoax\n",
      "0_pro-bot_sents: randomized, self, fiasco, apologizes, rite\n",
      "0_pro-human_sents: randomized, self, fiasco, placebo, volunteered\n",
      "\n",
      "mama\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: #bbcafricaeye, yasin, fallen, kasi, standing\n",
      "0_pro-human_sents: #bbcafricaeye, yasin, fallen, chair, standing\n",
      "\n",
      "father\n",
      "0_anti-bot_sents: declines, lincoln, paralyzed, od, anil\n",
      "0_anti-human_sents: mh, valuable, hr, od, dialysis\n",
      "0_pro-bot_sents: paralyzed, eric, lincoln, tirelessly, loss\n",
      "0_pro-human_sents: geoffrey, grieving, paralyzed, tirelessly, lincoln\n",
      "\n",
      "dad\n",
      "0_anti-bot_sents: wigan, load, vaxed, malaysian, viral\n",
      "0_anti-human_sents: wigan, vaxed, load, daughter, malaysian\n",
      "0_pro-bot_sents: vaxed, load, icu, malaysian, fi\n",
      "0_pro-human_sents: load, vaxed, malaysian, delhi, viral\n",
      "\n",
      "papa\n",
      "0_anti-bot_sents: presentent, officielle, emergence, projet, avant\n",
      "0_anti-human_sents: presentent, officielle, projet, emergence, avant\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: presentent, projet, jours, quand, avant\n",
      "\n",
      "trans\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: #indonesia, kuala, 30am, Ã¼ber, gibt\n",
      "0_pro-bot_sents: mountain, #moonshot, preuve, depuis, kits\n",
      "0_pro-human_sents: undiluted, chicagoans, shelters, attack, preuve\n",
      "\n",
      "transgender\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "queer\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "gay\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: #humor, #writer, #satire, #losangeles, #life\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: bashing, sab, hoga, cleric, iranian\n",
      "\n",
      "lesbian\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "bisexual\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "age\n",
      "children\n",
      "0_anti-bot_sents: cad, homicides, gujarati, autumn, 159\n",
      "0_anti-human_sents: unaff, inated, #vaccinationforchildren, autumn, attacking\n",
      "0_pro-bot_sents: homicides, cad, instruction, gujarati, 12-18\n",
      "0_pro-human_sents: homicides, cad, 1955, inno, extending\n",
      "\n",
      "kid\n",
      "0_anti-bot_sents: solely, throwing, teen, som, yep\n",
      "0_anti-human_sents: teen, som, #goodnews, solely, billionaire\n",
      "0_pro-bot_sents: nys, solely, vet, throwing, epivaccorona\n",
      "0_pro-human_sents: solely, epivaccorona, connected, throwing, billionaire\n",
      "\n",
      "child\n",
      "0_anti-bot_sents: lalwani, gÃ©nÃ©ral, 411009, appel, parent\n",
      "0_anti-human_sents: gÃ©nÃ©ral, vaxers, frankly, appel, lif\n",
      "0_pro-bot_sents: catch-up, chil, rounds, barrier, paralyzed\n",
      "0_pro-human_sents: kid's, catch-up, maternal, postnatal, rubella\n",
      "\n",
      "young\n",
      "0_anti-bot_sents: olympian, â‰¤, vazi, _ste, chronicling\n",
      "0_anti-human_sents: olympian, #vaccineforyouth, nature's, â‰¤, vazi\n",
      "0_pro-bot_sents: olympian, criticize, yr-olds, appointmen, exte\n",
      "0_pro-human_sents: olympian, yr-olds, exte, jessie, chasers\n",
      "\n",
      "youth\n",
      "0_anti-bot_sents: #covidhelp, assured, fails, 12-17, durham\n",
      "0_anti-human_sents: #wrdsb, assured, 12-17, fails, #covidhelp\n",
      "0_pro-bot_sents: rumours, connect, #punjabgovernment, network, tribune\n",
      "0_pro-human_sents: phones, parental, shelter, corps, 12-17\n",
      "\n",
      "old\n",
      "0_anti-bot_sents: pre-existing, risk-benefit, availabl, startup, jailed\n",
      "0_anti-human_sents: atm, goalkeeper, futsal, phyo, aung\n",
      "0_pro-bot_sents: 16-59, philly's, grandparents, startup, navigate\n",
      "0_pro-human_sents: fashioned, philly's, vying, 16-59, startup\n",
      "\n",
      "elderly\n",
      "0_anti-bot_sents: excuses, quasi-ineffective, anti-maskers, chickens, frail\n",
      "0_anti-human_sents: excuses, quasi-ineffective, facilitating, frail, #sindhgovt\n",
      "0_pro-bot_sents: chickens, multi-centric, carers, incorrect, #oann\n",
      "0_pro-human_sents: chickens, vot, multi-centric, researches, #oann\n",
      "\n",
      "aged\n",
      "0_anti-bot_sents: #alluarjun, cad, 40-49, gujarati, quotin\n",
      "0_anti-human_sents: cad, quotin, gujarati, 65-69, 40-49\n",
      "0_pro-bot_sents: 40-49, cad, gujarati, 12-18, georgians\n",
      "0_pro-human_sents: cad, 40-49, gujarati, 12-18, georgians\n",
      "\n",
      "grandma\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: pissed, lovely, 83, she's, wanna\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: pissed, 83, lovely, she's, wanna\n",
      "\n",
      "grandpa\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "grandfather\n",
      "0_anti-bot_sents: declined, rapidly, admitted, friends, checked\n",
      "0_anti-human_sents: declined, rapidly, admitted, friends, tested\n",
      "0_pro-bot_sents: \n"
     ]
    }
   ],
   "source": [
    "for cat in identities:\n",
    "    print(cat)\n",
    "    for term in identities[cat]:\n",
    "        print(term)\n",
    "        for name in processed:\n",
    "            outstring = ', '.join([el[0] for el in top_pmi(term, combination_word_freqs[name], cooccurrences[name], total_combinations[name])[:5]])\n",
    "            print(f'{name}: {outstring}')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
